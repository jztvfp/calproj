{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Present Pipeline\n",
    "\n",
    "Present the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "source_table = spark.conf.get(\"source_table\")\n",
    "target_table = spark.conf.get(\"target_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf66f040-7fbe-4d27-aa11-2b54869e30b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"project\"\n",
    "br_schema = \"bronze\"\n",
    "\n",
    "#Return the rules matching the tag as a format ready for DLT annotation.\n",
    "\n",
    "def get_rules(tablename, quality):\n",
    "  \"\"\"\n",
    "    loads data quality rules from csv file\n",
    "    :param tablename: tablename to match\n",
    "    :param quality: logical quality level\n",
    "    :return: dictionary of rules that matched the tag\n",
    "  \"\"\"\n",
    "  rules = {}\n",
    "  df = spark.table(f\"{catalog}.{br_schema}.expectations\").where(f\"tablename = '{tablename}' and quality ='{quality}'\")\n",
    "  for row in df.collect():\n",
    "    rules[row['name']] = row['constraint']\n",
    "  return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"ReadingDate\")\n",
    "\n",
    "\n",
    "@dlt.view(name=\"silver_data_view\", comment=\"Raw data from the turbines\")\n",
    "def silver_data_view():\n",
    "    df = spark.read.table(source_table).select(\n",
    "        F.col(\"ReadingDate\"),\n",
    "        F.col(\"TurbineNumber\"),\n",
    "        F.col(\"WindDirection\"),\n",
    "        F.col(\"PowerOutput\"),\n",
    "        # smooth over missing values\n",
    "        F.when(\n",
    "            F.col(\"WindSpeed\").isNull(), \n",
    "            F.lag(F.col(\"WindSpeed\")).over(w)\n",
    "        ).otherwise(F.col(\"WindSpeed\")).alias(\"WindSpeed\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "@dlt.table(name=target_table, comment=\"Presentation layer turbine data\")\n",
    "def mv_turbine_data():\n",
    "    query = \"\"\"\n",
    "    WITH turbine_data AS\n",
    "        ( \n",
    "        SELECT\n",
    "            TurbineNumber,\n",
    "            ReadingDate,\n",
    "            WindSpeed,\n",
    "            WindDirection,\n",
    "            ROUND(AVG(PowerOutput) OVER (PARTITION BY TurbineNumber), 4) AS LongTermAveragePowerOutput,\n",
    "            ROUND(STDDEV(PowerOutput) OVER (PARTITION BY TurbineNumber), 4) AS LongTermStandardDeviationPowerOutput,\n",
    "            MIN(PowerOutput) OVER (\n",
    "            PARTITION BY TurbineNumber \n",
    "            ORDER BY ReadingDate \n",
    "            RANGE BETWEEN INTERVAL 1 DAY PRECEDING AND CURRENT ROW\n",
    "            ) AS MinimumPowerOutputPreviousDay,\n",
    "            MAX(PowerOutput) OVER (\n",
    "            PARTITION BY TurbineNumber \n",
    "            ORDER BY ReadingDate \n",
    "            RANGE BETWEEN INTERVAL 1 DAY PRECEDING AND CURRENT ROW\n",
    "            ) AS MaximumPowerOutputPreviousDay,\n",
    "            ROUND(AVG(PowerOutput) OVER (\n",
    "            PARTITION BY TurbineNumber \n",
    "            ORDER BY ReadingDate \n",
    "            RANGE BETWEEN INTERVAL 1 DAY PRECEDING AND CURRENT ROW\n",
    "            ), 2) AS AveragePowerOutputPreviousDay\n",
    "        FROM \n",
    "            live.silver_data_view\n",
    "        )\n",
    "        SELECT \n",
    "        *,\n",
    "        ROUND(ABS(AveragePowerOutputPreviousDay - LongTermAveragePowerOutput), 4) AS PowerOutputDeviationFromLongTermAverage,\n",
    "        CASE \n",
    "            WHEN AveragePowerOutputPreviousDay - LongTermAveragePowerOutput > 2 * LongTermStandardDeviationPowerOutput \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END AS IsAnomaly \n",
    "        FROM \n",
    "        turbine_data\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_gold",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
